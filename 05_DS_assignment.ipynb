{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "856f9dee-9ac2-4c0c-9666-82eb2e0df1cc",
   "metadata": {},
   "source": [
    "#Q1\n",
    "\n",
    "A naive classifier model is one that does not use any sophistication in order to make a prediction, typically making a random or constant prediction. Such models are naive because they don't use any knowledge about the domain or any learning in order to make a prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb70ac10-eb82-4f01-98a8-4128c757ade9",
   "metadata": {},
   "source": [
    "#Q2\n",
    "\n",
    "It is a classification technique based on Bayes' Theorem with an independence assumption among predictors. In simple terms, a Naive Bayes classifier assumes that the presence of a particular feature in a class is unrelated to the presence of any other feature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894d48be-4624-47cc-b6d6-2e7964ec8fe2",
   "metadata": {},
   "source": [
    "#Q3\n",
    "\n",
    "Naïve Bayes Imputation (NBI) is used to fill in missing values by replacing the attribute information according to the probability estimate. The NBI process divides the whole data into two sub-sets is the complete data and data containing missing data. Complete data is used for the imputation process at the lost value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53407a04-8e93-4afc-83f3-681d73635a57",
   "metadata": {},
   "source": [
    "#Q4\n",
    "\n",
    "The advantage is that it is inexpensive to develop, store data, and operate. The disadvantage is that it does not consider any possible causal relationships that underly the forecasted variable. This model adds the latest observed absolute period -to-period change to the most recent observed level of the variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1707703d-639c-4339-b7a8-18d1562c6321",
   "metadata": {},
   "source": [
    "#Q5\n",
    "\n",
    "Naive Bayes is a supervised classification algorithm that is used primarily for dealing with binary and multi-class classification problems, though with some modifications, it can also be used for solving regression problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85a1ea9-c8ab-4577-a9ba-c124f6ed3823",
   "metadata": {},
   "source": [
    "#Q6\n",
    "\n",
    "Drop Categorical Variables The easiest approach to dealing with categorical variables is to simply remove them from the dataset. This approach will only work well if the columns did not contain useful information.\n",
    "Label Encoding Label encoding assigns each unique value to a different integer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c87240-90e5-46b4-8c05-03c42742360b",
   "metadata": {},
   "source": [
    "#Q7\n",
    "\n",
    "Conclusion. Laplace smoothing is a smoothing technique that helps tackle the problem of zero probability in the Naïve Bayes machine learning algorithm. Using higher alpha values will push the likelihood towards a value of 0.5, i.e., the probability of a word equal to 0.5 for both the positive and negative reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f259b8f-80bb-4170-84c0-645e2f0f9d55",
   "metadata": {},
   "source": [
    "#Q8\n",
    "\n",
    "To keep it simple, in the case of binary classification, you can set the thresholds as a value in the range [0, 1] , such that they sum to 1 . This will get you the desired rule of \"Classify as True if the probability is over threshold T, otherwise classify as False\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55dc2e22-f01c-4ecc-904e-374725e5b344",
   "metadata": {},
   "source": [
    "#Q9\n",
    "\n",
    "For example, you sold 250 computers last month and estimate that you would sell 250 computers again this month.\n",
    "\n",
    "The naive approach takes into account what happened in the preceding period and forecasts that it will happen again.\n",
    "\n",
    "Naive approach forecasting example\n",
    "Despite its simplicity, this strategy works remarkably effectively in practice.\n",
    "\n",
    "This article shows you how to do naive forecasting in R with step-by-step instructions.\n",
    "\n",
    "Step 1: Enter the Information\n",
    "To begin, we’ll enter sales data for a fictional corporation during a 12-month period.\n",
    "\n",
    "Make a vector to hold real-world sales data\n",
    "\n",
    "actual <- c(31, 35, 36, 33, 32, 31, 36, 35, 34, 32, 31, 32)\n",
    "Step 2: Generate the Naive Forecasts\n",
    "Then, for each month, we’ll make naive projections using the algorithms below.\n",
    "\n",
    "forecast <-\n",
    "Let’s view the naive forecasts\n",
    "\n",
    "forecast\n",
    "NA 31 35 36 33 32 31 36 35 34 32 31\n",
    "Note that for the first anticipated value, we just used NA.\n",
    "\n",
    "Step 3: Assess the Predictions’ Accuracy\n",
    "Finally, we must assess the forecasts’ precision. The following are two popular accuracy metrics:\n",
    "\n",
    "The absolute percentage error in the mean (MAPE)\n",
    "\n",
    "Absolute Mean Error (MAE)\n",
    "To calculate both metrics, we may use the following code:\n",
    "\n",
    "MAPE should be calculated.\n",
    "\n",
    "mean(abs((actual-forecast)/actual), na.rm=T) * 100\n",
    "5.630553\n",
    "Now we can calculate MAE\n",
    "\n",
    "mean(abs(actual-forecast), na.rm=T)\n",
    "1.909091\n",
    "The average absolute percentage inaccuracy is 5.6 percent, with an average absolute error of 1.9 percent.\n",
    "\n",
    "We may compare this forecast to other forecasting models to check if the accuracy measurements are better or worse to see whether it is useful.\n",
    "\n",
    "Step 4: Visualization\n",
    "\n",
    "Finally, we can display the discrepancies between actual sales and naive sales projections for each period using a simple line plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f159dab9-74fc-46fa-a8d9-cadefbe18917",
   "metadata": {},
   "source": [
    "#Q10\n",
    "\n",
    "The k-nearest neighbors algorithm, also known as KNN or k-NN, is a non-parametric, supervised learning classifier, which uses proximity to make classifications or predictions about the grouping of an individual data point."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779e098a-7f08-4205-be73-ce4eb46b2d94",
   "metadata": {},
   "source": [
    "#Q11\n",
    "\n",
    "K-NN algorithm assumes the similarity between the new case/data and available cases and put the new case into the category that is most similar to the available categories. K-NN algorithm stores all the available data and classifies a new data point based on the similarity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6978c1-057e-4331-b254-8858ca8a6011",
   "metadata": {},
   "source": [
    "#Q12\n",
    "\n",
    "The optimal K value usually found is the square root of N, where N is the total number of samples. Use an error plot or accuracy plot to find the most favorable K value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639918a7-6b0e-46a2-b392-3f70ccc5aecd",
   "metadata": {},
   "source": [
    "#Q13\n",
    "\n",
    "Some Advantages of KNN\n",
    "Quick calculation time.\n",
    "Simple algorithm – to interpret.\n",
    "Versatile – useful for regression and classification.\n",
    "High accuracy – you do not need to compare with better-supervised learning models.\n",
    "Some Disadvantages of KNN\n",
    "Accuracy depends on the quality of the data.\n",
    "With large data, the prediction stage might be slow.\n",
    "Sensitive to the scale of the data and irrelevant features.\n",
    "Require high memory – need to store all of the training data.\n",
    "Given that it stores all of the training, it can be computationally expensive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885da5d6-343b-4578-9ace-4750b7f4e7c6",
   "metadata": {},
   "source": [
    "#Q14\n",
    "\n",
    "In addition, the performance of the KNN with this top performing distance degraded only ∼20% while the noise level reaches 90%, this is true for most of the distances used as well. This means that the KNN classifier using any of the top 10 distances tolerates noise to a certain degree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91be1838-e423-4fbe-a3ca-9c1a29ff5782",
   "metadata": {},
   "source": [
    "#Q15\n",
    "\n",
    "In k Nearest Neighbor (kNN) classifier, a query instance is classified based on the most frequent class of its nearest neighbors among the training instances. In imbalanced datasets, kNN becomes biased towards the majority instances of the training space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce329bf-9085-4c0b-bf2c-b5ac01a908c5",
   "metadata": {},
   "source": [
    "#Q16\n",
    "\n",
    "You have to decide how to convert categorical features to a numeric scale, and somehow assign inter-category distances in a way that makes sense with other features (like, age-age distances...but what is an age-category distance?)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152e4775-7dd0-4970-b56b-8375c00000f4",
   "metadata": {},
   "source": [
    "#Q17\n",
    "\n",
    "Reduce the dimensionality\n",
    "In order to improve the efficiency and speed of KNN, reducing the dimensionality of the data is necessary. The curse of dimensionality can make the distance between data points less distinguishable and increase complexity for the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c663f189-cab2-4c31-b986-f45dd6a37839",
   "metadata": {},
   "source": [
    "#Q18\n",
    "\n",
    "With the help of KNN algorithms, we can classify a potential voter into various classes like “Will Vote”, “Will not Vote”, “Will Vote to Party 'Congress', “Will Vote to Party 'BJP'. Other areas in which KNN algorithm can be used are Speech Recognition, Handwriting Detection, Image Recognition and Video Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2984328f-a0c6-47c5-8ec2-8b7bf3f14d66",
   "metadata": {},
   "source": [
    "#Q19\n",
    "\n",
    "In machine learning too, we often group examples as a first step to understand a subject (data set) in a machine learning system. Grouping unlabeled examples is called clustering. As the examples are unlabeled, clustering relies on unsupervised machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd0b753-0ec5-4206-b65f-be12c28b6fec",
   "metadata": {},
   "source": [
    "#Q20\n",
    "\n",
    "k-means is method of cluster analysis using a pre-specified no. of clusters. It requires advance knowledge of 'K'. Hierarchical clustering also known as hierarchical cluster analysis (HCA) is also a method of cluster analysis which seeks to build a hierarchy of clusters without having fixed number of cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3fea443-5685-4b45-b763-b4661e02023c",
   "metadata": {},
   "source": [
    "#Q21\n",
    "\n",
    "The optimal number of clusters can be defined as follow:\n",
    "Compute clustering algorithm (e.g., k-means clustering) for different values of k. ...\n",
    "For each k, calculate the total within-cluster sum of square (wss).\n",
    "Plot the curve of wss according to the number of clusters k.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af18ef9c-0830-4ab6-8e4e-724bbf9407dd",
   "metadata": {},
   "source": [
    "#Q22\n",
    "\n",
    "Distance metrics are used in supervised and unsupervised learning to calculate similarity in data points. They improve the performance, whether that's for classification tasks or clustering. The four types of distance metrics are Euclidean Distance, Manhattan Distance, Minkowski Distance, and Hamming Distance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef614bf8-fa18-4338-89a5-7f2ab49e207e",
   "metadata": {},
   "source": [
    "#Q23\n",
    "\n",
    "Step 1: Pick K observations at random and use them as leaders/clusters.\n",
    "Step 2: Calculate the dissimilarities(no. of mismatches) and assign each observation to its closest cluster.\n",
    "Step 3: Define new modes for the clusters.\n",
    "Creating Toy Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2fa25e3-c14b-43f9-84b4-bddbcaffa8b9",
   "metadata": {},
   "source": [
    "#Q24\n",
    "\n",
    "Advantages of Hierarchical clustering\n",
    "It is simple to implement and gives the best output in some cases.\n",
    "It is easy and results in a hierarchy, a structure that contains more information.\n",
    "It does not need us to pre-specify the number of clusters.\n",
    "Disadvantages of hierarchical clustering\n",
    "It breaks the large clusters. It is Difficult to handle different sized clusters and convex shapes. It is sensitive to noise and outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1990d7-0550-4eb1-a9f1-b56755a939cb",
   "metadata": {},
   "source": [
    "#Q25\n",
    "\n",
    "The silhouette value is a measure of how similar an object is to its own cluster (cohesion) compared to other clusters (separation). The silhouette ranges from −1 to +1, where a high value indicates that the object is well matched to its own cluster and poorly matched to neighboring clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4991cb-e88e-4d67-aba9-3e34832145e9",
   "metadata": {},
   "source": [
    "#Q26\n",
    "\n",
    "Retail Marketing\n",
    "Retail companies often use clustering to identify groups of households that are similar to each other.\n",
    "\n",
    "For example, a retail company may collect the following information on households:\n",
    "\n",
    "Household income\n",
    "Household size\n",
    "Head of household Occupation\n",
    "Distance from nearest urban area\n",
    "They can then feed these variables into a clustering algorithm to perhaps identify the following clusters:\n",
    "Cluster 1: Small family, high spenders\n",
    "Cluster 2: Larger family, high spenders\n",
    "Cluster 3: Small family, low spenders\n",
    "Cluster 4: Large family, low spenders\n",
    "The company can then send personalized advertisements or sales letters to each household based on how likely they are to respond to specific types of advertisements.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b5b413-a941-47a0-b9f6-b91627b0c514",
   "metadata": {},
   "source": [
    "#Q27\n",
    "\n",
    "Anomaly detection is a process of finding those rare items, data points, events, or observations that make suspicions by being different from the rest data points or observations. Anomaly detection is also known as outlier detection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ec7a81-0790-4062-9c60-1293ebff9ebf",
   "metadata": {},
   "source": [
    "#Q28\n",
    "\n",
    "The main difference between supervised and unsupervised anomaly detection is the approach involved, where supervised approach makes use of predefined algorithms and AI training, while unsupervised approach uses a general outlier-detection mechanism based on pattern matching."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525f0a76-d55a-4ea2-b744-377baa372310",
   "metadata": {},
   "source": [
    "#Q29\n",
    "\n",
    "\n",
    "Some of the popular techniques are:\n",
    "Statistical (Z-score, Tukey's range test and Grubbs's test)\n",
    "Density-based techniques (k-nearest neighbor, local outlier factor, isolation forests, and many more variations of this concept)\n",
    "Subspace-, correlation-based and tensor-based outlier detection for high-dimensional data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd389642-451f-4c11-a3df-eb6fd24daf42",
   "metadata": {},
   "source": [
    "#Q30\n",
    "\n",
    "One-class SVM, or unsupervised SVM, is an algorithm used for anomaly detection. The algorithm tries to separate data from the origin in the transformed high-dimensional predictor space. ocsvm finds the decision boundary based on the primal form of SVM with the Gaussian kernel approximation method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01492e4c-270e-4fea-adae-abfce73eb62d",
   "metadata": {},
   "source": [
    "#Q31\n",
    "\n",
    "The way to tune the anomaly detection threshold is as follows:\n",
    "Construct a train set using a large sample of observations without anomalies.\n",
    "Take a smaller sample of observations containing anomalies (manually labelled) and use it to construct a validation and test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a50138-a899-4f85-80f4-417d4ea32eb0",
   "metadata": {},
   "source": [
    "#Q32\n",
    "\n",
    "Random Undersampling and Oversampling\n",
    "A widely adopted and perhaps the most straightforward method for dealing with highly imbalanced datasets is called resampling. It consists of removing samples from the majority class (under-sampling) and/or adding more examples from the minority class (over-sampling)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf7bcb6-7f75-4c9d-96fb-89e3c50bab21",
   "metadata": {},
   "source": [
    "#Q33\n",
    "\n",
    "For example, a credit card company will use anomaly detection to track how customers typically use their credit cards. If a customer makes an abnormally large purchase or a purchase in a new location, the algorithm recognizes the anomaly and alerts a team member to contact the customer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c09e102-2f21-44fa-845f-9375affa5461",
   "metadata": {},
   "source": [
    "#Q34\n",
    "\n",
    "Dimensionality reduction is the task of reducing the number of features in a dataset. In machine learning tasks like regression or classification, there are often too many variables to work with. These variables are also called features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4c9c70-5505-4f4c-b095-e821dad76aa6",
   "metadata": {},
   "source": [
    "#Q35\n",
    "\n",
    "Feature selection techniques are used when model explainability is a key requirement. Feature extraction techniques can be used to improve the predictive performance of the models, especially, in the case of algorithms that don't support regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8a66f3-8321-4072-ae0f-437b01a82014",
   "metadata": {},
   "source": [
    "#Q36\n",
    "\n",
    "PCA helps us to identify patterns in data based on the correlation between features. In a nutshell, PCA aims to find the directions of maximum variance in high-dimensional data and projects it onto a new subspace with equal or fewer dimensions than the original one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36816e2f-d1da-40c7-b11d-5b2334bbb037",
   "metadata": {},
   "source": [
    "#Q37\n",
    "\n",
    "If our sole intention of doing PCA is for data visualization, the best number of components is 2 or 3. If we really want to reduce the size of the dataset, the best number of principal components is much less than the number of variables in the original dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9499f6c-2ef4-41c6-8947-934198afa015",
   "metadata": {},
   "source": [
    "#Q38\n",
    "\n",
    "Dimensionality Reduction Techniques\n",
    "Feature selection. ...\n",
    "Feature extraction. ...\n",
    "Principal Component Analysis (PCA) ...\n",
    "Non-negative matrix factorization (NMF) ...\n",
    "Linear discriminant analysis (LDA) ...\n",
    "Generalized discriminant analysis (GDA) ...\n",
    "Missing Values Ratio. ...\n",
    "Low Variance Filter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f640f20-41bd-4253-93f2-649af8ac683b",
   "metadata": {},
   "source": [
    "#Q39\n",
    "\n",
    "For example, maybe we can combine Dum Dums and Blow Pops to look at all lollipops together. Dimensionality reduction can help in both of these scenarios. There are two key methods of dimensionality reduction: Feature selection: Here, we select a subset of features from the original feature set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072f3bd4-93f0-4134-89ee-22834cecf153",
   "metadata": {},
   "source": [
    "#Q40\n",
    "\n",
    "What is Feature Selection? Feature Selection is the method of reducing the input variable to your model by using only relevant data and getting rid of noise in data. It is the process of automatically choosing relevant features for your machine learning model based on the type of problem you are trying to solve."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b661fe7b-1054-4272-9871-8be66497b1fe",
   "metadata": {},
   "source": [
    "#Q41\n",
    "\n",
    "Filter methods perform the feature selection independently of construction of the classification model. Wrapper methods iteratively select or eliminate a set of features using the prediction accuracy of the classification model. In embedded methods the feature selection is an integral part of the classification model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1dd029-bda2-4891-a0a6-4b8131148d49",
   "metadata": {},
   "source": [
    "#Q42\n",
    "\n",
    "How does correlation help in feature selection? Features with high correlation are more linearly dependent and hence have almost the same effect on the dependent variable. So, when two features have high correlation, we can drop one of the two features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c374cfb7-573e-45d0-a4d9-75510a58d05e",
   "metadata": {},
   "source": [
    "#Q43\n",
    "\n",
    "To address multicollinearity, techniques such as regularization or feature selection can be applied to select a subset of independent variables that are not highly correlated with each other. In this article, we will focus on the most common one – VIF (Variance Inflation Factors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac0ac9c-2e2a-4ee4-ba1a-71f653309d32",
   "metadata": {},
   "source": [
    "#Q44\n",
    "\n",
    "Types of Feature Selection Methods in ML\n",
    "Chi-square Test. The Chi-square test is used for categorical features in a dataset. ...\n",
    "Fisher's Score. ...\n",
    "Correlation Coefficient. ...\n",
    "Dispersion Ratio. ...\n",
    "Backward Feature Elimination. ...\n",
    "Recursive Feature Elimination. ...\n",
    "Random Forest Importance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b55d6e-ae33-44ed-8e69-3608ae280ce7",
   "metadata": {},
   "source": [
    "#Q45\n",
    "\n",
    "Feature Selection is the method of reducing the input variable to your model by using only relevant data and getting rid of noise in data. It is the process of automatically choosing relevant features for your machine learning model based on the type of problem you are trying to solve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b2483a-cea0-4339-92df-ccdb8cc8617d",
   "metadata": {},
   "source": [
    "#Q46\n",
    "\n",
    "What is data drift? Data drift is one of the top reasons model accuracy degrades over time. For machine learning models, data drift is the change in model input data that leads to model performance degradation. Monitoring data drift helps detect these model performance issues."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4acd1e9b-7d77-4c11-94a2-43dc7cb2e397",
   "metadata": {},
   "source": [
    "#Q47\n",
    "\n",
    "Data drift is one of the top reasons model accuracy degrades over time. For machine learning models, data drift is the change in model input data that leads to model performance degradation. Monitoring data drift helps detect these model performance issues"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce63bc1d-f62a-41c6-911a-17fb0c592f48",
   "metadata": {},
   "source": [
    "#Q48\n",
    "\n",
    "Data drift refers to the changing distribution of the data to which the model is applied. Concept drift refers to a changing underlying goal or objective for the model. Both data drift and concept drift can lead to a decline in the performance of a machine learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479aa625-384f-4b90-9053-75dcb3843f64",
   "metadata": {},
   "source": [
    "#Q49\n",
    "\n",
    "Time distribution-based methods use statistical methods to calculate the difference between two probability distributions to detect drift. These methods include the Population Stability Index, KL Divergence, JS Divergence, KS Test, and the Wasserstein Metric."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8652e42-4dcd-46c6-8eda-d178214f8790",
   "metadata": {},
   "source": [
    "#Q50\n",
    "\n",
    "Some strategies for addressing drift include continuously monitoring and evaluating the performance of a model, updating the model with new data, and using machine learning models that are more robust to drift."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c94d9a-0626-4cd3-9324-91ced986ea1a",
   "metadata": {},
   "source": [
    "#Q51\n",
    "\n",
    "The unauthorized transmission of data from an organization to any external source is known as data leakage. This data can be leaked physically or electronically via hard drives, USB devices, mobile phones, etc., and could be exposed publicly or fall into the hands of a cyber criminal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d2a12f-89c8-4227-95d4-95609bfed269",
   "metadata": {},
   "source": [
    "#Q52\n",
    "\n",
    "A data leak is when information is exposed to unauthorized people due to internal errors. This is often caused by poor data security and sanitization, outdated systems, or a lack of employee training. Data leaks could lead to identity theft, data breaches, or ransomware installation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4631141f-18f3-40de-abd9-88b3b715e4b3",
   "metadata": {},
   "source": [
    " #Q53\n",
    " \n",
    "Data leakage refers to a situation where information from the target variable is inadvertently leaked into the training data, giving the model access to information that it wouldn't have in a real-world scenario when making predictions. This can result in inflated performance during training and validation stages but poor performance when the model is deployed in a production environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ca1744-c1d2-4194-8427-0deb6c9844fb",
   "metadata": {},
   "source": [
    "#Q54\n",
    "\n",
    "One of the best ways to get rid of data leakage is to perform k-fold cross validation where the overall data is divided into k parts. After dividing into k parts, we use each part as the cross-validation data and the remaining as training data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3005e8-69ae-4aaa-ac45-fc1215f1a700",
   "metadata": {},
   "source": [
    "#Q55\n",
    "\n",
    "8 Most Common Causes of Data Breach\n",
    "Weak and Stolen Credentials, a.k.a. Passwords. ...\n",
    "Back Doors, Application Vulnerabilities. ...\n",
    "Malware. ...\n",
    "Social Engineering. ...\n",
    "Too Many Permissions. ...\n",
    "Insider Threats. ...\n",
    "Physical Attacks. ...\n",
    "Improper Configuration, User Error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568e3ea6-e8f7-4ea8-aa41-3ed0f699dd74",
   "metadata": {},
   "source": [
    "#Q56\n",
    "\n",
    "One example scenario where data leakage can occur is in a company that handles sensitive customer information, such as personal details and financial data. Suppose the company has implemented strict security measures to protect this information, including access controls and encryption. However, an employee who has authorized access to this data may inadvertently or maliciously leak it.\n",
    "\n",
    "For instance, let's say an employee who has legitimate access to the customer database decides to copy a portion of the data onto a personal device or email it to a personal email account. This could happen due to various reasons, such as intending to use the data for personal gain or accidentally sending it to the wrong recipient.\n",
    "\n",
    "Once the data is outside the company's secure environment, it becomes vulnerable to unauthorized access, hacking, or misuse. If the leaked data contains sensitive information like social security numbers, credit card details, or medical records, it can lead to identity theft, financial fraud, or other harmful consequences for the individuals whose data has been compromised.\n",
    "\n",
    "Data leakage can also occur through unintentional means. For instance, an employee might accidentally leave a physical document or an unencrypted USB drive containing sensitive data in a public place or lose it during transit. In these situations, if someone else finds the misplaced information, it can result in a data breach and potential misuse.\n",
    "\n",
    "Overall, data leakage can occur in various ways, whether it's due to intentional actions by insiders or accidental incidents, posing significant risks to privacy, security, and trust. Organizations must implement robust security protocols, ongoing employee training, and monitoring mechanisms to prevent and mitigate the risk of data leakage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f34822b-7f1c-40cb-bdd6-70f206aea979",
   "metadata": {},
   "source": [
    "#Q57\n",
    "\n",
    "Cross-validation is a technique for evaluating ML models by training several ML models on subsets of the available input data and evaluating them on the complementary subset of the data. Use cross-validation to detect overfitting, ie, failing to generalize a pattern."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99eeae59-3613-437c-aee0-04219d12db1b",
   "metadata": {},
   "source": [
    "#Q58\n",
    "\n",
    "The main advantage of cross-validation is that it provides an estimate of the performance of the model on new data, which is important for assessing the model's generalizability. It also helps to avoid overfitting, which is a common problem in machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25466908-a3c4-4ad6-a85a-e2a4d5395bac",
   "metadata": {},
   "source": [
    "#Q59\n",
    "\n",
    "Stratified k-fold cross-validation is the same as just k-fold cross-validation, But Stratified k-fold cross-validation, it does stratified sampling instead of random sampling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d05b5c-1e3e-47d4-ba9f-9fda3fa59df9",
   "metadata": {},
   "source": [
    "#Q60\n",
    "\n",
    "Cross validation is a technique that allows us to produce test set like scoring metrics using the training set. That is, it allows us to simulate the effects of “going out of sample” using just our training data, so we can get a sense of how well our model generalizes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
